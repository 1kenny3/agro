#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üåΩ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –ü–†–ê–í–ò–õ–¨–ù–û–ì–û –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –ö–£–ö–£–†–£–ó–´
================================================================
–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π images.jpeg –∫–∞–∫ –∫—É–∫—É—Ä—É–∑–∞
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import numpy as np
import cv2
import json
from pathlib import Path
from sklearn.model_selection import train_test_split
import timm

class CornDataset(Dataset):
    def __init__(self, image_paths, labels, visual_features_list, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.visual_features_list = visual_features_list
        self.transform = transform
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
        self.class_to_idx = {'wheat': 0, 'corn': 1, 'barley': 2}
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image_path = self.image_paths[idx]
        image = Image.open(image_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫—É
        label = self.class_to_idx[self.labels[idx]]
        
        # –ü–æ–ª—É—á–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        visual_features = self.visual_features_list[idx]
        visual_tensor = torch.tensor([
            visual_features.get('green_ratio', 0),
            visual_features.get('yellow_ratio', 0),
            visual_features.get('vertical_lines', 0),
            visual_features.get('edge_density', 0),
            visual_features.get('aspect_ratio', 1)
        ], dtype=torch.float32)
        
        return image, visual_tensor, label

class EnhancedCropClassifier(nn.Module):
    def __init__(self, num_classes=3, model_name="efficientnet_b0", use_visual_features=True):
        super().__init__()
        
        self.use_visual_features = use_visual_features
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º timm –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, 224, 224)
            features = self.backbone(dummy_input)
            self.feature_size = features.shape[1]
        
        print(f"üîß –†–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ backbone: {self.feature_size}")
        
        # –°–µ—Ç—å –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if use_visual_features:
            self.visual_net = nn.Sequential(
                nn.Linear(5, 32),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(32, 64),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(64, 32)
            )
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            self.classifier = nn.Sequential(
                nn.Linear(self.feature_size + 32, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(128, num_classes)
            )
        else:
            self.classifier = nn.Sequential(
                nn.Linear(self.feature_size, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, num_classes)
            )
    
    def forward(self, images, visual_features=None):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_features = self.backbone(images)
        
        if self.use_visual_features and visual_features is not None:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            visual_features = self.visual_net(visual_features)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            combined_features = torch.cat([image_features, visual_features], dim=1)
        else:
            combined_features = image_features
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        output = self.classifier(combined_features)
        return output

def create_corrected_dataset():
    """–°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–æ–π"""
    print("üìä –°–û–ó–î–ê–ù–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
    with open("photo_analysis_results.json", 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    image_paths = []
    labels = []
    visual_features_list = []
    
    for result in results:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏
        if result.get('visual_features', {}).get('error'):
            continue
        
        file_path = result['file_info']['path']
        filename = Path(file_path).name.lower()
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –†–ê–ó–ú–ï–¢–ö–ê
        if 'images.jpeg' in filename:
            # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑–∞–ª, —á—Ç–æ —ç—Ç–æ –∫—É–∫—É—Ä—É–∑–∞
            correct_label = 'corn'
            print(f"üåΩ {filename} -> –ö–£–ö–£–†–£–ó–ê (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ)")
        elif '119252' in filename:
            correct_label = 'corn'
            print(f"üåΩ {filename} -> –∫—É–∫—É—Ä—É–∑–∞")
        elif any(word in filename for word in ['wheat', '–ø—à–µ–Ω–∏—Ü–∞', '–ø—à–µ–Ω–∏—Ü']):
            correct_label = 'wheat'
            print(f"üåæ {filename} -> –ø—à–µ–Ω–∏—Ü–∞")
        elif filename.startswith('dji_'):
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º DJI —Ñ–æ—Ç–æ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
            visual_features = result['visual_features']
            yellow_ratio = visual_features.get('yellow_ratio', 0)
            green_ratio = visual_features.get('green_ratio', 0)
            
            if yellow_ratio > 0.3 and green_ratio < 0.1:
                if any(num in filename for num in ['0046', '0048', '0031']):
                    correct_label = 'wheat'
                    print(f"üåæ {filename} -> –ø—à–µ–Ω–∏—Ü–∞")
                else:
                    correct_label = 'barley'
                    print(f"üåø {filename} -> —è—á–º–µ–Ω—å")
            else:
                correct_label = 'barley'
                print(f"üåø {filename} -> —è—á–º–µ–Ω—å")
        else:
            correct_label = 'barley'
            print(f"üåø {filename} -> —è—á–º–µ–Ω—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)")
        
        image_paths.append(file_path)
        labels.append(correct_label)
        visual_features_list.append(result['visual_features'])
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    class_counts = {}
    for label in labels:
        class_counts[label] = class_counts.get(label, 0) + 1
    
    print(f"\nüìä –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for cls, count in class_counts.items():
        class_ru = {'wheat': '–ø—à–µ–Ω–∏—Ü–∞', 'corn': '–∫—É–∫—É—Ä—É–∑–∞', 'barley': '—è—á–º–µ–Ω—å'}[cls]
        print(f"   {class_ru}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    return image_paths, labels, visual_features_list

def train_corrected_model():
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    print("\nüöÄ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –° –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    image_paths, labels, visual_features_list = create_corrected_dataset()
    
    if len(image_paths) < 5:
        print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
    if len(set(labels)) >= 2 and len(image_paths) >= 8:
        train_paths, val_paths, train_labels, val_labels, train_features, val_features = train_test_split(
            image_paths, labels, visual_features_list, test_size=0.2, random_state=42, stratify=labels
        )
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        train_paths, train_labels, train_features = image_paths, labels, visual_features_list
        val_paths, val_labels, val_features = image_paths[:2], labels[:2], visual_features_list[:2]
    
    print(f"üìä –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = CornDataset(train_paths, train_labels, train_features, transform=train_transform)
    val_dataset = CornDataset(val_paths, val_labels, val_features, transform=val_transform)
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    model = EnhancedCropClassifier(num_classes=3, use_visual_features=True)
    model.to(device)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    
    # –û–±—É—á–µ–Ω–∏–µ
    num_epochs = 30
    best_acc = 0.0
    
    print("\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    
    for epoch in range(num_epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        for images, visual_features, labels in train_loader:
            images = images.to(device)
            visual_features = visual_features.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images, visual_features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        train_acc = 100 * train_correct / train_total
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, visual_features, labels in val_loader:
                images = images.to(device)
                visual_features = visual_features.to(device)
                labels = labels.to(device)
                
                outputs = model(images, visual_features)
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        val_acc = 100 * val_correct / val_total
        
        print(f"–≠–ø–æ—Ö–∞ [{epoch+1}/{num_epochs}] - Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if val_acc > best_acc:
            best_acc = val_acc
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é {best_acc:.2f}%")
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            Path("data/models").mkdir(parents=True, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            torch.save({
                'model_state_dict': model.state_dict(),
                'best_accuracy': best_acc,
                'class_to_idx': train_dataset.class_to_idx,
                'epoch': epoch
            }, "data/models/enhanced_crop_classifier.pth")
    
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_acc:.2f}%")
    print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: data/models/enhanced_crop_classifier.pth")

if __name__ == "__main__":
    train_corrected_model() 