#!/usr/bin/env python3
"""
–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫—É–ª—å—Ç—É—Ä
–Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∏–∑ –ø–∞–ø–∫–∏ photo/
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import json
import numpy as np
from pathlib import Path
import cv2
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

from src.models.crop_classifier import CropClassifier
from src.config.settings import settings

class CropPhotoDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –∫—É–ª—å—Ç—É—Ä —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏"""
    
    def __init__(self, image_paths, labels, visual_features_list, transform=None, use_features=True):
        self.image_paths = image_paths
        self.labels = labels
        self.visual_features_list = visual_features_list
        self.transform = transform
        self.use_features = use_features
        
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–ª–∞—Å—Å–æ–≤
        self.class_to_idx = {'wheat': 0, 'corn': 1, 'barley': 2}
        self.idx_to_class = {0: 'wheat', 1: 'corn', 2: 'barley'}
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image_path = self.image_paths[idx]
        try:
            image = Image.open(image_path).convert('RGB')
        except Exception as e:
            # –°–æ–∑–¥–∞–µ–º —á–µ—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å
            image = Image.new('RGB', (224, 224), color=(0, 0, 0))
        
        if self.transform:
            image = self.transform(image)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫—É
        label = self.class_to_idx.get(self.labels[idx], 2)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —è—á–º–µ–Ω—å
        
        # –í–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        if self.use_features and idx < len(self.visual_features_list):
            features = self.visual_features_list[idx]
            visual_tensor = torch.tensor([
                features.get('green_ratio', 0.0),
                features.get('yellow_ratio', 0.0),
                features.get('vertical_lines', 0.0),
                features.get('edge_density', 0.0),
                features.get('aspect_ratio', 1.0)
            ], dtype=torch.float32)
        else:
            visual_tensor = torch.zeros(5, dtype=torch.float32)
        
        return image, visual_tensor, label

class EnhancedCropClassifier(nn.Module):
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å —É—á–µ—Ç–æ–º –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    
    def __init__(self, num_classes=3, model_name="efficientnet_b0", use_visual_features=True):
        super(EnhancedCropClassifier, self).__init__()
        
        self.use_visual_features = use_visual_features
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        import timm
        self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)  # num_classes=0 –¥–ª—è feature extraction
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, 224, 224)
            features = self.backbone(dummy_input)
            self.feature_size = features.shape[1]
        
        print(f"üîß –†–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ backbone: {self.feature_size}")
        
        # –°–µ—Ç—å –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if use_visual_features:
            self.visual_net = nn.Sequential(
                nn.Linear(5, 32),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(32, 64),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(64, 32)
            )
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            self.classifier = nn.Sequential(
                nn.Linear(self.feature_size + 32, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, 128),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(128, num_classes)
            )
        else:
            self.classifier = nn.Sequential(
                nn.Linear(self.feature_size, 256),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(256, num_classes)
            )
    
    def forward(self, images, visual_features=None):
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_features = self.backbone(images)
        
        if self.use_visual_features and visual_features is not None:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            visual_features = self.visual_net(visual_features)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            combined_features = torch.cat([image_features, visual_features], dim=1)
        else:
            combined_features = image_features
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        output = self.classifier(combined_features)
        return output

def load_analyzed_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    results_file = "photo_analysis_results.json"
    
    if not Path(results_file).exists():
        print(f"‚ùå –§–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ {results_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python analyze_photos.py")
        return None, None, None
    
    with open(results_file, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    image_paths = []
    labels = []
    visual_features_list = []
    
    for result in results:
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã —Å –æ—à–∏–±–∫–∞–º–∏
        if result.get('visual_features', {}).get('error'):
            continue
            
        file_path = result['file_info']['path']
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –º–µ—Ç–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
        manual_class = result['manual_identification']['class']
        manual_confidence = result['manual_identification']['confidence']
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        if manual_confidence > 0.3:
            image_paths.append(file_path)
            labels.append(manual_class)
            visual_features_list.append(result['visual_features'])
    
    print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(image_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
    class_counts = {}
    for label in labels:
        class_counts[label] = class_counts.get(label, 0) + 1
    
    print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–ª–∞—Å—Å–∞–º:")
    for cls, count in class_counts.items():
        class_ru = {'wheat': '–ø—à–µ–Ω–∏—Ü–∞', 'corn': '–∫—É–∫—É—Ä—É–∑–∞', 'barley': '—è—á–º–µ–Ω—å'}.get(cls, cls)
        print(f"   {class_ru}: {count} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    return image_paths, labels, visual_features_list

def create_data_augmentation():
    """–°–æ–∑–¥–∞–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    return {
        'train': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(15),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]),
        'val': transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    }

def train_model():
    """–û–±—É—á–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –£–õ–£–ß–®–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –ö–£–õ–¨–¢–£–†")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    image_paths, labels, visual_features_list = load_analyzed_data()
    
    if not image_paths:
        return None
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    if len(image_paths) < 10:
        print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
        print("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ...")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö –∫–ª–∞—Å—Å–æ–≤
        image_paths, labels, visual_features_list = generate_synthetic_data(
            image_paths, labels, visual_features_list
        )
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    if len(set(labels)) < 2:
        print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è –≤ –∫–ª–∞—Å—Å–∞—Ö. –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã...")
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        image_paths, labels, visual_features_list = add_base_examples(
            image_paths, labels, visual_features_list
        )
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    train_paths, val_paths, train_labels, val_labels, train_features, val_features = train_test_split(
        image_paths, labels, visual_features_list, test_size=0.2, random_state=42, stratify=labels
    )
    
    print(f"üìä –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    print(f"üìä –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(val_paths)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    transforms_dict = create_data_augmentation()
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    train_dataset = CropPhotoDataset(
        train_paths, train_labels, train_features, 
        transform=transforms_dict['train'], use_features=True
    )
    
    val_dataset = CropPhotoDataset(
        val_paths, val_labels, val_features,
        transform=transforms_dict['val'], use_features=True
    )
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    device = torch.device(settings.DEVICE)
    model = EnhancedCropClassifier(num_classes=3, use_visual_features=True)
    model.to(device)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)
    
    # –û–±—É—á–µ–Ω–∏–µ
    num_epochs = 20
    best_val_acc = 0.0
    
    train_losses = []
    val_accuracies = []
    
    print("\nüéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
    
    for epoch in range(num_epochs):
        # –û–±—É—á–µ–Ω–∏–µ
        model.train()
        running_loss = 0.0
        correct_train = 0
        total_train = 0
        
        for images, visual_features, labels in train_loader:
            images = images.to(device)
            visual_features = visual_features.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images, visual_features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()
        
        train_acc = 100 * correct_train / total_train
        avg_train_loss = running_loss / len(train_loader)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        model.eval()
        correct_val = 0
        total_val = 0
        
        with torch.no_grad():
            for images, visual_features, labels in val_loader:
                images = images.to(device)
                visual_features = visual_features.to(device)
                labels = labels.to(device)
                
                outputs = model(images, visual_features)
                _, predicted = torch.max(outputs, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()
        
        val_acc = 100 * correct_val / total_val
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
        scheduler.step(1 - val_acc/100)
        
        train_losses.append(avg_train_loss)
        val_accuracies.append(val_acc)
        
        print(f"–≠–ø–æ—Ö–∞ [{epoch+1}/{num_epochs}] - "
              f"Train Loss: {avg_train_loss:.4f}, "
              f"Train Acc: {train_acc:.2f}%, "
              f"Val Acc: {val_acc:.2f}%")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            model_save_path = "data/models/enhanced_crop_classifier.pth"
            Path(model_save_path).parent.mkdir(parents=True, exist_ok=True)
            
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': epoch,
                'val_acc': val_acc,
                'class_to_idx': train_dataset.class_to_idx
            }, model_save_path)
            
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é {val_acc:.2f}%")
    
    print(f"\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_val_acc:.2f}%")
    
    # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    plot_training_history(train_losses, val_accuracies)
    
    return model, best_val_acc

def generate_synthetic_data(image_paths, labels, visual_features_list):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–≤–µ–ª–∏—á–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print("üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –∫–ª–∞—Å—Å—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
    present_classes = set(labels)
    all_classes = {'wheat', 'corn', 'barley'}
    missing_classes = all_classes - present_classes
    
    synthetic_paths = []
    synthetic_labels = []
    synthetic_features = []
    
    for missing_class in missing_classes:
        # –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∫–ª–∞—Å—Å–∞
        for i in range(5):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            if missing_class == 'wheat':
                features = {
                    'green_ratio': np.random.uniform(0.1, 0.4),
                    'yellow_ratio': np.random.uniform(0.3, 0.7),
                    'vertical_lines': np.random.uniform(0.2, 0.4),
                    'edge_density': np.random.uniform(0.15, 0.25),
                    'aspect_ratio': np.random.uniform(0.7, 1.3)
                }
            elif missing_class == 'corn':
                features = {
                    'green_ratio': np.random.uniform(0.5, 0.8),
                    'yellow_ratio': np.random.uniform(0.1, 0.3),
                    'vertical_lines': np.random.uniform(0.4, 0.6),
                    'edge_density': np.random.uniform(0.1, 0.2),
                    'aspect_ratio': np.random.uniform(1.2, 1.8)
                }
            else:  # barley
                features = {
                    'green_ratio': np.random.uniform(0.3, 0.6),
                    'yellow_ratio': np.random.uniform(0.2, 0.4),
                    'vertical_lines': np.random.uniform(0.1, 0.3),
                    'edge_density': np.random.uniform(0.12, 0.22),
                    'aspect_ratio': np.random.uniform(0.8, 1.2)
                }
            
            synthetic_paths.append(f"synthetic_{missing_class}_{i}")
            synthetic_labels.append(missing_class)
            synthetic_features.append(features)
    
    print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(synthetic_paths)} —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    return image_paths + synthetic_paths, labels + synthetic_labels, visual_features_list + synthetic_features

def add_base_examples(image_paths, labels, visual_features_list):
    """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è"""
    print("üìù –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤...")
    
    # –ë–∞–∑–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    base_examples = {
        'wheat': {
            'green_ratio': 0.25,
            'yellow_ratio': 0.5,
            'vertical_lines': 0.3,
            'edge_density': 0.2,
            'aspect_ratio': 1.0
        },
        'corn': {
            'green_ratio': 0.65,
            'yellow_ratio': 0.2,
            'vertical_lines': 0.5,
            'edge_density': 0.15,
            'aspect_ratio': 1.5
        },
        'barley': {
            'green_ratio': 0.45,
            'yellow_ratio': 0.3,
            'vertical_lines': 0.2,
            'edge_density': 0.18,
            'aspect_ratio': 1.1
        }
    }
    
    present_classes = set(labels)
    all_classes = {'wheat', 'corn', 'barley'}
    
    for class_name in all_classes:
        if class_name not in present_classes:
            image_paths.append(f"base_example_{class_name}")
            labels.append(class_name)
            visual_features_list.append(base_examples[class_name])
    
    return image_paths, labels, visual_features_list

def plot_training_history(train_losses, val_accuracies):
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –æ–±—É—á–µ–Ω–∏—è"""
    try:
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
        ax1.plot(train_losses)
        ax1.set_title('Training Loss')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        ax2.plot(val_accuracies)
        ax2.set_title('Validation Accuracy')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.grid(True)
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
        print("üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ training_history.png")
        
    except Exception as e:
        print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏: {e}")

if __name__ == "__main__":
    print("üåæ –°–ò–°–¢–ï–ú–ê –û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ò –ù–ê –†–ï–ê–õ–¨–ù–´–• –§–û–¢–û–ì–†–ê–§–ò–Ø–•")
    print("üéØ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ç–æ –∏–∑ –ø–∞–ø–∫–∏ photo/")
    print("=" * 65)
    
    try:
        model, best_accuracy = train_model()
        
        if model and best_accuracy:
            print(f"\nüéâ –û–ë–£–ß–ï–ù–ò–ï –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û!")
            print(f"üèÜ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {best_accuracy:.2f}%")
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: data/models/enhanced_crop_classifier.pth")
            print(f"üìä –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è: training_history.png")
            print("\n‚úÖ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
        else:
            print("‚ùå –û–±—É—á–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≤–µ—Ä—à–∏—Ç—å")
            
    except Exception as e:
        print(f"üí• –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc() 