import torch
import torch.nn as nn
import timm
from typing import Dict, List, Tuple, Optional
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from pathlib import Path
import cv2
import torch.nn.functional as F
from transformers import AutoImageProcessor, AutoModelForImageClassification
import warnings
warnings.filterwarnings('ignore')

from ..config.settings import settings

class AdvancedCropClassifier(nn.Module):
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–µ–ª—å—Å–∫–æ—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞–º–∏"""
    
    def __init__(self, num_classes: int = 3, model_name: str = "swin_base_patch4_window7_224", pretrained: bool = True):
        super(AdvancedCropClassifier, self).__init__()
        
        self.num_classes = num_classes
        self.model_name = model_name
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        self.backbone = timm.create_model(
            model_name,
            pretrained=pretrained,
            num_classes=num_classes
        )
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        if "swin" in model_name:
            # –î–ª—è Swin Transformer –º–æ–¥–µ–ª–µ–π
            self.feature_size = self.backbone.head.in_features
            self.backbone.head = nn.Sequential(
                nn.LayerNorm(self.feature_size),
                nn.Dropout(0.3),
                nn.Linear(self.feature_size, 512),
                nn.GELU(),
                nn.Dropout(0.2),
                nn.Linear(512, num_classes)
            )
        elif "vit" in model_name:
            # –î–ª—è Vision Transformer –º–æ–¥–µ–ª–µ–π
            self.feature_size = self.backbone.head.in_features
            self.backbone.head = nn.Sequential(
                nn.LayerNorm(self.feature_size),
                nn.Dropout(0.2),
                nn.Linear(self.feature_size, 256),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(256, num_classes)
            )
        elif "efficientnetv2" in model_name:
            # –î–ª—è EfficientNetV2 –º–æ–¥–µ–ª–µ–π
            self.feature_size = self.backbone.classifier.in_features
            self.backbone.classifier = nn.Sequential(
                nn.Dropout(0.4),
                nn.Linear(self.feature_size, 512),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(512, 256),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(256, num_classes)
            )
        elif "convnext" in model_name:
            # –î–ª—è ConvNeXt –º–æ–¥–µ–ª–µ–π
            if hasattr(self.backbone.head, 'fc'):
                self.feature_size = self.backbone.head.fc.in_features
                self.backbone.head.fc = nn.Sequential(
                    nn.LayerNorm(self.feature_size),
                    nn.Dropout(0.3),
                    nn.Linear(self.feature_size, num_classes)
                )
            else:
                self.feature_size = self.backbone.head.in_features
                self.backbone.head = nn.Sequential(
                    nn.LayerNorm(self.feature_size),
                    nn.Dropout(0.3),
                    nn.Linear(self.feature_size, num_classes)
                )
        else:
            # –î–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π (ResNet, EfficientNet –∏ —Ç.–¥.)
            if hasattr(self.backbone, 'classifier'):
                self.feature_size = self.backbone.classifier.in_features
                self.backbone.classifier = nn.Sequential(
                    nn.Dropout(0.3),
                    nn.Linear(self.feature_size, 512),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(512, num_classes)
                )
            elif hasattr(self.backbone, 'fc'):
                self.feature_size = self.backbone.fc.in_features
                self.backbone.fc = nn.Sequential(
                    nn.Dropout(0.3),
                    nn.Linear(self.feature_size, 512),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(512, num_classes)
                )
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """–ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å"""
        return self.backbone(x)
    
    def extract_features(self, x: torch.Tensor) -> torch.Tensor:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±–µ–∑ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        if "swin" in self.model_name or "vit" in self.model_name:
            return self.backbone.forward_features(x)
        elif "convnext" in self.model_name:
            features = self.backbone.forward_features(x)
            if hasattr(self.backbone.head, 'global_pool'):
                return self.backbone.head.global_pool(features)
            return features.mean(dim=[-2, -1])  # Global average pooling
        else:
            features = self.backbone.forward_features(x)
            if hasattr(self.backbone, 'global_pool'):
                return self.backbone.global_pool(features)
            return features.mean(dim=[-2, -1])  # Global average pooling

class NextGenCropClassifier:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–æ–≤–æ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏"""
    
    def __init__(self, device: str = None):
        self.device = device or settings.DEVICE
        self.classes = settings.CROP_CLASSES
        self.classes_ru = settings.CROP_CLASSES_RU
        
        # –°–æ–∑–¥–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å –∏–∑ —Å–∞–º—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.models = self._create_next_gen_ensemble()
        
        # –í–µ—Å–∞ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è (–º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
        self.ensemble_weights = {
            "Swin-Transformer": 0.35,
            "Vision-Transformer": 0.30,
            "EfficientNetV2": 0.25,
            "ConvNeXt-V2": 0.10
        }
        
    def _create_next_gen_ensemble(self) -> List[Tuple[str, AdvancedCropClassifier]]:
        """–°–æ–∑–¥–∞–µ–º –∞–Ω—Å–∞–º–±–ª—å –∏–∑ —Å–∞–º—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        models = []
        
        # 1. Swin Transformer - –ª—É—á—à–∏–π –¥–ª—è —Å–µ–ª—å—Å–∫–æ—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é Swin Transformer - —Ä–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å—Ç–µ–Ω–∏–π...")
        try:
            swin_model = AdvancedCropClassifier(
                num_classes=len(self.classes),
                model_name="swin_base_patch4_window7_224",
                pretrained=True
            )
            swin_model.to(self.device)
            swin_model.eval()
            models.append(("Swin-Transformer", swin_model))
            print("‚úÖ Swin Transformer —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω! –ì–æ—Ç–æ–≤ –∫ –∞–Ω–∞–ª–∏–∑—É —Å—Ç—Ä—É–∫—Ç—É—Ä —Ä–∞—Å—Ç–µ–Ω–∏–π")
        except Exception as e:
            print(f"‚ö†Ô∏è Swin Transformer –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e}")
            # Fallback –Ω–∞ –º–µ–Ω—å—à—É—é –≤–µ—Ä—Å–∏—é
            try:
                swin_tiny = AdvancedCropClassifier(
                    num_classes=len(self.classes),
                    model_name="swin_tiny_patch4_window7_224",
                    pretrained=True
                )
                swin_tiny.to(self.device)
                swin_tiny.eval()
                models.append(("Swin-Transformer-Tiny", swin_tiny))
                print("‚úÖ Swin Transformer Tiny –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤!")
            except Exception as e2:
                print(f"‚ö†Ô∏è Swin Transformer Tiny —Ç–æ–∂–µ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e2}")
        
        # 2. Vision Transformer - –æ—Ç–ª–∏—á–Ω—ã–π –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é Vision Transformer - –ø–µ—Ä–µ–¥–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è...")
        try:
            vit_model = AdvancedCropClassifier(
                num_classes=len(self.classes),
                model_name="vit_base_patch16_224",
                pretrained=True
            )
            vit_model.to(self.device)
            vit_model.eval()
            models.append(("Vision-Transformer", vit_model))
            print("‚úÖ Vision Transformer –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ! –ü—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        except Exception as e:
            print(f"‚ö†Ô∏è Vision Transformer –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e}")
            # Fallback –Ω–∞ –º–µ–Ω—å—à—É—é –≤–µ—Ä—Å–∏—é
            try:
                vit_small = AdvancedCropClassifier(
                    num_classes=len(self.classes),
                    model_name="vit_small_patch16_224",
                    pretrained=True
                )
                vit_small.to(self.device)
                vit_small.eval()
                models.append(("Vision-Transformer-Small", vit_small))
                print("‚úÖ Vision Transformer Small –∑–∞–≥—Ä—É–∂–µ–Ω!")
            except Exception as e2:
                print(f"‚ö†Ô∏è Vision Transformer Small —Ç–æ–∂–µ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e2}")
        
        # 3. EfficientNetV2 - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è EfficientNet
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é EfficientNetV2 - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏...")
        try:
            efficientv2_model = AdvancedCropClassifier(
                num_classes=len(self.classes),
                model_name="efficientnetv2_s",
                pretrained=True
            )
            efficientv2_model.to(self.device)
            efficientv2_model.eval()
            models.append(("EfficientNetV2", efficientv2_model))
            print("‚úÖ EfficientNetV2 –≥–æ—Ç–æ–≤! –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        except Exception as e:
            print(f"‚ö†Ô∏è EfficientNetV2 –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e}")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π EfficientNet
            try:
                efficient_model = AdvancedCropClassifier(
                    num_classes=len(self.classes),
                    model_name="efficientnet_b3",
                    pretrained=True
                )
                efficient_model.to(self.device)
                efficient_model.eval()
                models.append(("EfficientNet-B3", efficient_model))
                print("‚úÖ EfficientNet-B3 –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤!")
            except Exception as e2:
                print(f"‚ö†Ô∏è EfficientNet-B3 —Ç–æ–∂–µ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e2}")
        
        # 4. ConvNeXt V2 - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è CNN –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
        print("üöÄ –ó–∞–≥—Ä—É–∂–∞—é ConvNeXt V2 - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é CNN –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É...")
        try:
            convnext_model = AdvancedCropClassifier(
                num_classes=len(self.classes),
                model_name="convnextv2_tiny",
                pretrained=True
            )
            convnext_model.to(self.device)
            convnext_model.eval()
            models.append(("ConvNeXt-V2", convnext_model))
            print("‚úÖ ConvNeXt V2 –≥–æ—Ç–æ–≤! –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è CNN —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏")
        except Exception as e:
            print(f"‚ö†Ô∏è ConvNeXt V2 –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e}")
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π ConvNeXt
            try:
                convnext_model = AdvancedCropClassifier(
                    num_classes=len(self.classes),
                    model_name="convnext_tiny",
                    pretrained=True
                )
                convnext_model.to(self.device)
                convnext_model.eval()
                models.append(("ConvNeXt", convnext_model))
                print("‚úÖ ConvNeXt –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤!")
            except Exception as e2:
                print(f"‚ö†Ô∏è ConvNeXt —Ç–æ–∂–µ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e2}")
        
        if not models:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å! –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π ResNet50...")
            # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤ - ResNet50
            try:
                resnet_model = AdvancedCropClassifier(
                    num_classes=len(self.classes),
                    model_name="resnet50",
                    pretrained=True
                )
                resnet_model.to(self.device)
                resnet_model.eval()
                models.append(("ResNet50-Fallback", resnet_model))
                print("‚úÖ ResNet50 –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤")
            except Exception as e:
                print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –¥–∞–∂–µ ResNet50 –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è: {e}")
        
        print(f"\nüéØ –ê–Ω—Å–∞–º–±–ª—å –≥–æ—Ç–æ–≤! –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π:")
        for name, _ in models:
            print(f"   ‚Ä¢ {name}")
        
        return models
    
    def _get_advanced_transform(self) -> transforms.Compose:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return transforms.Compose([
            transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])
    
    def _analyze_crop_morphology(self, image: Image.Image) -> Dict:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∞—Å—Ç–µ–Ω–∏–π"""
        img_array = np.array(image.convert('RGB'))
        
        # –ê–Ω–∞–ª–∏–∑ —Ü–≤–µ—Ç–æ–≤—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        hsv = cv2.cvtColor(img_array, cv2.COLOR_RGB2HSV)
        lab = cv2.cvtColor(img_array, cv2.COLOR_RGB2LAB)
        
        # –ê–Ω–∞–ª–∏–∑ –∑–µ–ª–µ–Ω–æ–≥–æ –∫–∞–Ω–∞–ª–∞ (—Ö–ª–æ—Ä–æ—Ñ–∏–ª–ª)
        green_channel = img_array[:, :, 1]
        green_intensity = np.mean(green_channel) / 255.0
        green_std = np.std(green_channel) / 255.0
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç—É—Ä—ã –ª–∏—Å—Ç—å–µ–≤
        gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        
        # –õ–æ–∫–∞–ª—å–Ω—ã–µ –±–∏–Ω–∞—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç—É—Ä—ã
        def calculate_lbp_variance(image, radius=3):
            """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
            kernel = np.ones((radius*2+1, radius*2+1), np.uint8)
            mean_filtered = cv2.filter2D(image.astype(np.float32), -1, kernel/(radius*2+1)**2)
            variance = cv2.filter2D((image.astype(np.float32) - mean_filtered)**2, -1, kernel/(radius*2+1)**2)
            return np.mean(variance)
        
        texture_variance = calculate_lbp_variance(gray)
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–∞—Å—Ç–µ–Ω–∏—è
        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        
        # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
        angles = np.arctan2(sobely, sobelx)
        vertical_lines = np.sum(np.abs(angles) < np.pi/6) / angles.size
        horizontal_lines = np.sum(np.abs(angles - np.pi/2) < np.pi/6) / angles.size
        
        # –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º—ã –ª–∏—Å—Ç—å–µ–≤
        # –ü–æ–∏—Å–∫ –∫–æ–Ω—Ç—É—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ä–º—ã
        edges = cv2.Canny(gray, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # –ê–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω –ª–∏—Å—Ç—å–µ–≤
        aspect_ratios = []
        leaf_areas = []
        
        for contour in contours:
            if cv2.contourArea(contour) > 100:  # –§–∏–ª—å—Ç—Ä—É–µ–º –º–µ–ª–∫–∏–µ –∫–æ–Ω—Ç—É—Ä—ã
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = h / w if w > 0 else 0
                aspect_ratios.append(aspect_ratio)
                leaf_areas.append(cv2.contourArea(contour))
        
        avg_aspect_ratio = np.mean(aspect_ratios) if aspect_ratios else 1.0
        total_leaf_area = sum(leaf_areas)
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∫—É–∫—É—Ä—É–∑—ã
        corn_indicators = {
            "tassel_presence": self._detect_corn_tassel(hsv),
            "broad_leaves": avg_aspect_ratio > 2.0,
            "vertical_structure": vertical_lines > 0.3,
            "characteristic_green": 0.3 < green_intensity < 0.7
        }
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø—à–µ–Ω–∏—Ü—ã
        wheat_indicators = {
            "golden_color": self._detect_wheat_color(hsv),
            "thin_structure": avg_aspect_ratio > 3.0,
            "high_texture": texture_variance > 100,
            "uniform_pattern": green_std < 0.15
        }
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è —è—á–º–µ–Ω—è
        barley_indicators = {
            "awns_presence": self._detect_barley_awns(gray),
            "compact_structure": avg_aspect_ratio < 2.5,
            "medium_texture": 50 < texture_variance < 150,
            "characteristic_color": self._detect_barley_color(hsv)
        }
        
        return {
            "green_intensity": green_intensity,
            "green_std": green_std,
            "texture_variance": texture_variance,
            "vertical_lines": vertical_lines,
            "horizontal_lines": horizontal_lines,
            "avg_aspect_ratio": avg_aspect_ratio,
            "total_leaf_area": total_leaf_area,
            "corn_score": sum(corn_indicators.values()) / len(corn_indicators),
            "wheat_score": sum(wheat_indicators.values()) / len(wheat_indicators),
            "barley_score": sum(barley_indicators.values()) / len(barley_indicators),
            "corn_indicators": corn_indicators,
            "wheat_indicators": wheat_indicators,
            "barley_indicators": barley_indicators
        }
    
    def _detect_corn_tassel(self, hsv_image: np.ndarray) -> bool:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –º–µ—Ç–µ–ª–æ–∫ –∫—É–∫—É—Ä—É–∑—ã"""
        upper_region = hsv_image[:hsv_image.shape[0]//3, :, :]
        # –¶–≤–µ—Ç–æ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –¥–ª—è –º–µ—Ç–µ–ª–æ–∫ (–∂–µ–ª—Ç–æ-–∫–æ—Ä–∏—á–Ω–µ–≤—ã–π)
        tassel_mask = cv2.inRange(upper_region, 
                                 np.array([10, 50, 50]), 
                                 np.array([30, 255, 255]))
        tassel_ratio = np.sum(tassel_mask > 0) / tassel_mask.size
        return tassel_ratio > 0.05
    
    def _detect_wheat_color(self, hsv_image: np.ndarray) -> bool:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ–ª–æ—Ç–∏—Å—Ç–æ–≥–æ —Ü–≤–µ—Ç–∞ –ø—à–µ–Ω–∏—Ü—ã"""
        # –ó–æ–ª–æ—Ç–∏—Å—Ç—ã–π —Ü–≤–µ—Ç –ø—à–µ–Ω–∏—Ü—ã
        golden_mask = cv2.inRange(hsv_image,
                                 np.array([15, 50, 100]),
                                 np.array([35, 255, 255]))
        golden_ratio = np.sum(golden_mask > 0) / golden_mask.size
        return golden_ratio > 0.1
    
    def _detect_barley_awns(self, gray_image: np.ndarray) -> bool:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ—Å—Ç–µ–π —è—á–º–µ–Ω—è"""
        # –ü–æ–∏—Å–∫ —Ç–æ–Ω–∫–∏—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å—Ç—Ä—É–∫—Ç—É—Ä (–æ—Å—Ç–µ–π)
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, 10))
        opened = cv2.morphologyEx(gray_image, cv2.MORPH_OPEN, kernel)
        awns_ratio = np.sum(opened > 0) / opened.size
        return awns_ratio > 0.05
    
    def _detect_barley_color(self, hsv_image: np.ndarray) -> bool:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞ —è—á–º–µ–Ω—è"""
        # –°–≤–µ—Ç–ª–æ-–∫–æ—Ä–∏—á–Ω–µ–≤—ã–π/–±–µ–∂–µ–≤—ã–π —Ü–≤–µ—Ç —è—á–º–µ–Ω—è
        barley_mask = cv2.inRange(hsv_image,
                                 np.array([8, 30, 80]),
                                 np.array([25, 180, 220]))
        barley_ratio = np.sum(barley_mask > 0) / barley_mask.size
        return barley_ratio > 0.08
    
    def predict(self, image: Image.Image) -> Dict:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        morphology = self._analyze_crop_morphology(image)
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        transform = self._get_advanced_transform()
        tensor = transform(image).unsqueeze(0).to(self.device)
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        ensemble_predictions = []
        model_results = {}
        model_confidences = {}
        
        for model_name, model in self.models:
            try:
                with torch.no_grad():
                    outputs = model(tensor)
                    probabilities = torch.softmax(outputs, dim=1)
                    probs_np = probabilities[0].cpu().numpy()
                    
                    model_results[model_name] = probs_np
                    model_confidences[model_name] = float(np.max(probs_np))
                    
                    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞ –∞–Ω—Å–∞–º–±–ª—è
                    weight = self.ensemble_weights.get(model_name, 0.25)
                    weighted_probs = probs_np * weight
                    ensemble_predictions.append(weighted_probs)
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
        
        if not ensemble_predictions:
            return self._create_error_result()
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ - –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É —Å –º–∞—Å—Å–∏–≤–∞–º–∏
        try:
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é —Ñ–æ—Ä–º—É
            ensemble_predictions = [pred for pred in ensemble_predictions if pred is not None and len(pred) == len(self.classes)]
            
            if not ensemble_predictions:
                return self._create_error_result()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤ –∏ —É—Å—Ä–µ–¥–Ω—è–µ–º
            ensemble_array = np.array(ensemble_predictions)
            final_probabilities = np.mean(ensemble_array, axis=0)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            final_probabilities = final_probabilities / np.sum(final_probabilities)
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–∏ –∞–Ω—Å–∞–º–±–ª—è: {e}")
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–≤–æ–π –º–æ–¥–µ–ª–∏
            if model_results:
                first_model_result = list(model_results.values())[0]
                final_probabilities = first_model_result / np.sum(first_model_result)
            else:
                return self._create_error_result()
        
        # –ë–∞–∑–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predicted_class_idx = np.argmax(final_probabilities)
        base_predicted_class = self.classes[predicted_class_idx]
        base_confidence = float(final_probabilities[predicted_class_idx])
        
        # –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–ê–Ø –ö–û–†–†–ï–ö–¶–ò–Ø –ù–ê –û–°–ù–û–í–ï –ú–û–†–§–û–õ–û–ì–ò–ò
        morphology_scores = {
            "corn": morphology["corn_score"],
            "wheat": morphology["wheat_score"], 
            "barley": morphology["barley_score"]
        }
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –º–∞—Ç—á
        best_morphology_match = max(morphology_scores.keys(), key=lambda k: morphology_scores[k])
        morphology_confidence = morphology_scores[best_morphology_match]
        
        # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è
        analysis_notes = []
        
        # –ï—Å–ª–∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è —Å–∏–ª—å–Ω–æ –Ω–µ —Å–æ–≥–ª–∞—Å–Ω–∞ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é –∏ –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if (best_morphology_match != base_predicted_class and 
            morphology_confidence > 0.7 and 
            base_confidence < 0.8):
            
            predicted_class = best_morphology_match
            predicted_class_ru = self.classes_ru[predicted_class]
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = (base_confidence + morphology_confidence) / 2
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            corrected_probs = final_probabilities.copy()
            morph_idx = self.classes.index(best_morphology_match)
            corrected_probs[morph_idx] = max(corrected_probs[morph_idx], confidence)
            corrected_probs = corrected_probs / np.sum(corrected_probs)
            
            final_probabilities = corrected_probs
            
            analysis_notes = [
                f"üî¨ –ú–û–†–§–û–õ–û–ì–ò–ß–ï–°–ö–ê–Ø –ö–û–†–†–ï–ö–¶–ò–Ø: {predicted_class_ru}",
                f"üß† –ù–µ–π—Ä–æ—Å–µ—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∏: {self.classes_ru[base_predicted_class]} ({base_confidence:.3f})",
                f"üîç –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞: {predicted_class_ru} ({morphology_confidence:.3f})",
                "‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è"
            ]
            
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –≤–æ–∑–º–æ–∂–Ω—ã–º —É—Å–∏–ª–µ–Ω–∏–µ–º
            predicted_class = base_predicted_class
            predicted_class_ru = self.classes_ru[predicted_class]
            
            # –ï—Å–ª–∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è —Å–æ–≥–ª–∞—Å–Ω–∞, –ø–æ–≤—ã—à–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            if best_morphology_match == base_predicted_class and morphology_confidence > 0.6:
                confidence = min(base_confidence + 0.1, 0.95)
                analysis_notes = [
                    f"‚úÖ –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û: {predicted_class_ru}",
                    f"üß† –ù–µ–π—Ä–æ—Å–µ—Ç–∏: {base_confidence:.3f}",
                    f"üîç –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—è: {morphology_confidence:.3f}",
                    "üéØ –ü–æ–ª–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"
                ]
            else:
                confidence = base_confidence
                analysis_notes = [
                    f"üß† –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: {predicted_class_ru}",
                    f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f}",
                    f"üîç –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞: {morphology_confidence:.3f}"
                ]
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        all_probabilities = {}
        all_probabilities_ru = {}
        
        for i, class_name in enumerate(self.classes):
            prob = float(final_probabilities[i])
            all_probabilities[class_name] = prob
            all_probabilities_ru[self.classes_ru[class_name]] = prob
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if confidence >= 0.85:
            confidence_level = "–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è"
            is_confident = True
        elif confidence >= 0.7:
            confidence_level = "–í—ã—Å–æ–∫–∞—è"
            is_confident = True
        elif confidence >= 0.55:
            confidence_level = "–°—Ä–µ–¥–Ω—è—è"
            is_confident = True
        elif confidence >= 0.4:
            confidence_level = "–ù–∏–∑–∫–∞—è"
            is_confident = False
        else:
            confidence_level = "–û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è"
            is_confident = False
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
        models_info = []
        for model_name, model_conf in model_confidences.items():
            models_info.append(f"{model_name}: {model_conf:.3f}")
        
        ensemble_info = f"ü§ñ –ê–Ω—Å–∞–º–±–ª—å –∏–∑ {len(self.models)} —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
        analysis_notes.insert(0, ensemble_info)
        analysis_notes.append(f"üìà –î–µ—Ç–∞–ª–∏ –º–æ–¥–µ–ª–µ–π: {', '.join(models_info)}")
        
        return {
            "predicted_class": predicted_class,
            "predicted_class_ru": predicted_class_ru,
            "confidence": confidence,
            "confidence_level": confidence_level,
            "confidence_gap": float(np.max(final_probabilities) - np.partition(final_probabilities, -2)[-2]),
            "probabilities": all_probabilities,
            "probabilities_ru": all_probabilities_ru,
            "is_confident": is_confident,
            "analysis_notes": analysis_notes,
            "morphology_analysis": morphology,
            "model_results": model_results,
            "ensemble_method": "weighted_voting_with_morphology"
        }
    
    def _create_error_result(self) -> Dict:
        """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ"""
        return {
            "predicted_class": "uncertain",
            "predicted_class_ru": "–Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–∏–ø",
            "confidence": 0.0,
            "confidence_level": "–û—à–∏–±–∫–∞",
            "probabilities": {cls: 0.33 for cls in self.classes},
            "probabilities_ru": {self.classes_ru[cls]: 0.33 for cls in self.classes},
            "is_confident": False,
            "analysis_notes": ["‚ùå –û—à–∏–±–∫–∞ –≤ —Ä–∞–±–æ—Ç–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∞–Ω—Å–∞–º–±–ª—è"],
            "morphology_analysis": {},
            "model_results": {},
            "ensemble_method": "error"
        } 