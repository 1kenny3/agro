"""
–ú–æ–¥—É–ª—å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import cv2
import numpy as np
from PIL import Image
import os
import json
from typing import Dict, List, Tuple
from pathlib import Path
import hashlib
from collections import defaultdict

from ..config.settings import settings

class DatasetValidator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    
    def __init__(self, dataset_path: str):
        self.dataset_path = Path(dataset_path)
        self.min_resolution = 512
        self.max_file_size = 5 * 1024 * 1024  # 5MB
        self.min_file_size = 100 * 1024  # 100KB
        self.blur_threshold = 100  # –õ–∞–ø–ª–∞—Å–∏–∞–Ω –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º—ã—Ç–æ—Å—Ç–∏
        
    def validate_image_quality(self, image_path: Path) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        results = {
            "path": str(image_path),
            "valid": True,
            "issues": [],
            "metrics": {}
        }
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            if not image_path.exists():
                results["valid"] = False
                results["issues"].append("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return results
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
            file_size = image_path.stat().st_size
            results["metrics"]["file_size"] = file_size
            
            if file_size > self.max_file_size:
                results["issues"].append(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {file_size/1024/1024:.1f}MB")
            elif file_size < self.min_file_size:
                results["issues"].append(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π: {file_size/1024:.1f}KB")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            try:
                image = Image.open(image_path)
                image_cv = cv2.imread(str(image_path))
            except Exception as e:
                results["valid"] = False
                results["issues"].append(f"–û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
                return results
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
            width, height = image.size
            results["metrics"]["resolution"] = (width, height)
            
            if min(width, height) < self.min_resolution:
                results["issues"].append(f"–ù–∏–∑–∫–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {width}x{height}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞
            if image.format not in ['JPEG', 'PNG']:
                results["issues"].append(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {image.format}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑–º—ã—Ç–æ—Å—Ç—å
            if image_cv is not None:
                gray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)
                blur_value = cv2.Laplacian(gray, cv2.CV_64F).var()
                results["metrics"]["blur_value"] = float(blur_value)
                
                if blur_value < self.blur_threshold:
                    results["issues"].append(f"–†–∞–∑–º—ã—Ç–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {blur_value:.1f}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–∫—Å–ø–æ–∑–∏—Ü–∏–∏
            if image_cv is not None:
                mean_brightness = np.mean(gray)
                results["metrics"]["brightness"] = float(mean_brightness)
                
                if mean_brightness < 50:
                    results["issues"].append("–°–ª–∏—à–∫–æ–º —Ç–µ–º–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
                elif mean_brightness > 200:
                    results["issues"].append("–°–ª–∏—à–∫–æ–º —Å–≤–µ—Ç–ª–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç–∏
            if image_cv is not None:
                contrast = np.std(gray)
                results["metrics"]["contrast"] = float(contrast)
                
                if contrast < 20:
                    results["issues"].append("–ù–∏–∑–∫–∏–π –∫–æ–Ω—Ç—Ä–∞—Å—Ç")
            
            if results["issues"]:
                results["valid"] = False
                
        except Exception as e:
            results["valid"] = False
            results["issues"].append(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        
        return results
    
    def find_duplicates(self, image_paths: List[Path]) -> Dict[str, List[str]]:
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ö–µ—à—É"""
        hashes = {}
        duplicates = defaultdict(list)
        
        for path in image_paths:
            try:
                with open(path, 'rb') as f:
                    file_hash = hashlib.md5(f.read()).hexdigest()
                
                if file_hash in hashes:
                    duplicates[file_hash].extend([str(hashes[file_hash]), str(path)])
                else:
                    hashes[file_hash] = path
                    
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {path}: {e}")
        
        return dict(duplicates)
    
    def validate_annotations(self, annotation_path: Path) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–π–ª–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π"""
        results = {
            "path": str(annotation_path),
            "valid": True,
            "issues": []
        }
        
        if not annotation_path.exists():
            results["valid"] = False
            results["issues"].append("–§–∞–π–ª –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return results
        
        try:
            with open(annotation_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
            required_fields = ['filename', 'crop_type', 'health_status']
            for field in required_fields:
                if field not in data:
                    results["issues"].append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ: {field}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏–π
            if 'crop_type' in data:
                if data['crop_type'] not in settings.CROP_CLASSES:
                    results["issues"].append(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫—É–ª—å—Ç—É—Ä—ã: {data['crop_type']}")
            
            if 'health_status' in data:
                valid_health = ['healthy', 'diseased', 'mixed']
                if data['health_status'] not in valid_health:
                    results["issues"].append(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å—Ç–∞—Ç—É—Å –∑–¥–æ—Ä–æ–≤—å—è: {data['health_status']}")
            
            if results["issues"]:
                results["valid"] = False
                
        except json.JSONDecodeError as e:
            results["valid"] = False
            results["issues"].append(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        
        return results
    
    def validate_dataset_structure(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        results = {
            "valid": True,
            "issues": [],
            "statistics": {}
        }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        required_dirs = ['raw', 'processed', 'annotations']
        for dir_name in required_dirs:
            dir_path = self.dataset_path / dir_name
            if not dir_path.exists():
                results["issues"].append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {dir_name}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫—É–ª—å—Ç—É—Ä–∞–º
        crop_stats = {}
        for crop in settings.CROP_CLASSES:
            crop_dir = self.dataset_path / 'raw' / crop
            if crop_dir.exists():
                images = list(crop_dir.glob('**/*.jpg')) + list(crop_dir.glob('**/*.png'))
                crop_stats[crop] = len(images)
            else:
                crop_stats[crop] = 0
                results["issues"].append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è {crop}")
        
        results["statistics"]["crop_counts"] = crop_stats
        results["statistics"]["total_images"] = sum(crop_stats.values())
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        min_images_per_crop = 100  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        for crop, count in crop_stats.items():
            if count < min_images_per_crop:
                results["issues"].append(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è {crop}: {count} < {min_images_per_crop}")
        
        if results["issues"]:
            results["valid"] = False
        
        return results
    
    def generate_quality_report(self) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print("üîç –ù–∞—á–∏–Ω–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        
        report = {
            "dataset_path": str(self.dataset_path),
            "validation_timestamp": np.datetime64('now').isoformat(),
            "structure_validation": {},
            "image_validation": {},
            "annotation_validation": {},
            "duplicates": {},
            "summary": {}
        }
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        print("üìÅ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        report["structure_validation"] = self.validate_dataset_structure()
        
        # 2. –ü–æ–∏—Å–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        image_extensions = ['*.jpg', '*.jpeg', '*.png']
        all_images = []
        for ext in image_extensions:
            all_images.extend(self.dataset_path.glob(f'**/{ext}'))
            all_images.extend(self.dataset_path.glob(f'**/{ext.upper()}'))
        
        print(f"üì∏ –ù–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        # 3. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        valid_images = 0
        invalid_images = 0
        all_issues = []
        
        image_results = []
        for i, img_path in enumerate(all_images):
            if i % 100 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
            
            result = self.validate_image_quality(img_path)
            image_results.append(result)
            
            if result["valid"]:
                valid_images += 1
            else:
                invalid_images += 1
                all_issues.extend(result["issues"])
        
        report["image_validation"] = {
            "total_images": len(all_images),
            "valid_images": valid_images,
            "invalid_images": invalid_images,
            "results": image_results
        }
        
        # 4. –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        print("üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
        duplicates = self.find_duplicates(all_images)
        report["duplicates"] = duplicates
        
        # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
        print("üìù –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π...")
        annotation_files = list(self.dataset_path.glob('**/*.json'))
        annotation_results = []
        valid_annotations = 0
        
        for ann_path in annotation_files:
            result = self.validate_annotations(ann_path)
            annotation_results.append(result)
            if result["valid"]:
                valid_annotations += 1
        
        report["annotation_validation"] = {
            "total_annotations": len(annotation_files),
            "valid_annotations": valid_annotations,
            "results": annotation_results
        }
        
        # 6. –°–≤–æ–¥–∫–∞
        report["summary"] = {
            "overall_valid": (
                report["structure_validation"]["valid"] and
                invalid_images == 0 and
                len(duplicates) == 0 and
                len(annotation_files) == valid_annotations
            ),
            "image_quality_score": valid_images / len(all_images) if all_images else 0,
            "annotation_completeness": valid_annotations / len(annotation_files) if annotation_files else 0,
            "duplicate_ratio": len(duplicates) / len(all_images) if all_images else 0,
            "recommendations": self._generate_recommendations(report)
        }
        
        print("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        return report
    
    def _generate_recommendations(self, report: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        recommendations = []
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        struct_issues = report["structure_validation"]["issues"]
        if struct_issues:
            recommendations.append("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –¥–∞—Ç–∞—Å–µ—Ç–∞")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        invalid_ratio = report["image_validation"]["invalid_images"] / report["image_validation"]["total_images"]
        if invalid_ratio > 0.1:
            recommendations.append(f"–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ({invalid_ratio:.1%}). –ù–µ–æ–±—Ö–æ–¥–∏–º–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è")
        
        # –ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        if len(report["duplicates"]) > 0:
            recommendations.append(f"–ù–∞–π–¥–µ–Ω–æ {len(report['duplicates'])} –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤. –£–¥–∞–ª–∏—Ç–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫—É–ª—å—Ç—É—Ä–∞–º
        crop_counts = report["structure_validation"]["statistics"]["crop_counts"]
        min_count = min(crop_counts.values()) if crop_counts else 0
        if min_count < 500:
            recommendations.append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –∫—É–ª—å—Ç—É—Ä. –¢—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        
        # –ë–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
        if crop_counts:
            max_count = max(crop_counts.values())
            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
            if imbalance_ratio > 3:
                recommendations.append("–°–∏–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤. –î–æ–±–∞–≤—å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –Ω–µ–¥–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –∫—É–ª—å—Ç—É—Ä")
        
        return recommendations

def validate_dataset_quality(dataset_path: str) -> Dict:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    validator = DatasetValidator(dataset_path)
    return validator.generate_quality_report()

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    dataset_path = settings.DATA_DIR
    report = validate_dataset_quality(str(dataset_path))
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_path = dataset_path / "quality_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"üìä –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {report_path}") 