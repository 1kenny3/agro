#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üåΩ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –ö–£–ö–£–†–£–ó–´
=======================================================
–ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ images.jpeg –∫–∞–∫ –∫—É–∫—É—Ä—É–∑—É
"""

import torch
import torch.nn as nn
from torchvision import transforms
from torchvision.models import efficientnet_b0
from PIL import Image
import numpy as np
import cv2
from pathlib import Path
import timm

class TrainedCornClassifier:
    def __init__(self, model_path="data/models/enhanced_crop_classifier.pth"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_path = model_path
        self.class_to_idx = {'wheat': 0, 'corn': 1, 'barley': 2}
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])
        
        self.model = self.load_model()
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {self.model_path}...")
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
            class EnhancedCropClassifier(nn.Module):
                def __init__(self, num_classes=3, model_name="efficientnet_b0", use_visual_features=True):
                    super().__init__()
                    
                    self.use_visual_features = use_visual_features
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º timm –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    self.backbone = timm.create_model(model_name, pretrained=True, num_classes=0)
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –≤—ã—Ö–æ–¥–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    with torch.no_grad():
                        dummy_input = torch.randn(1, 3, 224, 224)
                        features = self.backbone(dummy_input)
                        self.feature_size = features.shape[1]
                    
                    print(f"üîß –†–∞–∑–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ backbone: {self.feature_size}")
                    
                    # –°–µ—Ç—å –¥–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
                    if use_visual_features:
                        self.visual_net = nn.Sequential(
                            nn.Linear(5, 32),
                            nn.ReLU(),
                            nn.Dropout(0.2),
                            nn.Linear(32, 64),
                            nn.ReLU(),
                            nn.Dropout(0.2),
                            nn.Linear(64, 32)
                        )
                        
                        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
                        self.classifier = nn.Sequential(
                            nn.Linear(self.feature_size + 32, 256),
                            nn.ReLU(),
                            nn.Dropout(0.3),
                            nn.Linear(256, 128),
                            nn.ReLU(),
                            nn.Dropout(0.2),
                            nn.Linear(128, num_classes)
                        )
                    else:
                        self.classifier = nn.Sequential(
                            nn.Linear(self.feature_size, 256),
                            nn.ReLU(),
                            nn.Dropout(0.3),
                            nn.Linear(256, num_classes)
                        )
                
                def forward(self, images, visual_features=None):
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    image_features = self.backbone(images)
                    
                    if self.use_visual_features and visual_features is not None:
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                        visual_features = self.visual_net(visual_features)
                        
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                        combined_features = torch.cat([image_features, visual_features], dim=1)
                    else:
                        combined_features = image_features
                    
                    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
                    output = self.classifier(combined_features)
                    return output
            
            model = EnhancedCropClassifier(num_classes=3, use_visual_features=True)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
            checkpoint = torch.load(self.model_path, map_location=self.device)
            model.load_state_dict(checkpoint['model_state_dict'])
            
            model.to(self.device)
            model.eval()
            
            best_accuracy = checkpoint.get('best_accuracy', 0)
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞! –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {best_accuracy:.2f}%")
            
            return model
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def extract_visual_features(self, image_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = cv2.imread(str(image_path))
            if image is None:
                raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
            
            h, w = image.shape[:2]
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB –∏ HSV
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
            
            # 1. –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∑–µ–ª–µ–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞
            green_mask = cv2.inRange(image_hsv, (35, 40, 40), (85, 255, 255))
            green_ratio = np.sum(green_mask > 0) / (h * w)
            
            # 2. –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∂–µ–ª—Ç–æ–≥–æ —Ü–≤–µ—Ç–∞
            yellow_mask = cv2.inRange(image_hsv, (15, 40, 40), (35, 255, 255))
            yellow_ratio = np.sum(yellow_mask > 0) / (h * w)
            
            # 3. –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–µ –ª–∏–Ω–∏–∏ (Sobel —Ñ–∏–ª—å—Ç—Ä)
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            vertical_lines = np.mean(np.abs(sobel_y)) / (np.mean(np.abs(sobel_x)) + 1e-8)
            vertical_lines = min(vertical_lines, 2.0) / 2.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            
            # 4. –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –∫—Ä–∞–µ–≤
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / (h * w)
            
            # 5. –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω
            aspect_ratio = w / h
            
            features = torch.tensor([
                green_ratio,
                yellow_ratio, 
                vertical_lines,
                edge_density,
                aspect_ratio
            ], dtype=torch.float32)
            
            return features
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return torch.zeros(5, dtype=torch.float32)
    
    def predict(self, image_path):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if self.model is None:
            return None
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.open(image_path).convert('RGB')
            image_tensor = self.transform(image).unsqueeze(0).to(self.device)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            visual_features = self.extract_visual_features(image_path).unsqueeze(0).to(self.device)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                outputs = self.model(image_tensor, visual_features)
                probabilities = torch.softmax(outputs, dim=1)
                predicted_idx = torch.argmax(probabilities, dim=1).item()
                confidence = probabilities[0][predicted_idx].item()
            
            predicted_class = self.idx_to_class[predicted_idx]
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
            all_probabilities = {}
            for idx, class_name in self.idx_to_class.items():
                all_probabilities[class_name] = probabilities[0][idx].item()
            
            class_names_ru = {'wheat': '–ø—à–µ–Ω–∏—Ü–∞', 'corn': '–∫—É–∫—É—Ä—É–∑–∞', 'barley': '—è—á–º–µ–Ω—å'}
            
            return {
                'predicted_class': predicted_class,
                'predicted_class_ru': class_names_ru.get(predicted_class, predicted_class),
                'confidence': confidence,
                'probabilities': all_probabilities,
                'visual_features': {
                    'green_ratio': visual_features[0][0].item(),
                    'yellow_ratio': visual_features[0][1].item(),
                    'vertical_lines': visual_features[0][2].item(),
                    'edge_density': visual_features[0][3].item(),
                    'aspect_ratio': visual_features[0][4].item()
                }
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return None

def test_corn_image():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è images.jpeg"""
    print("üåΩ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –ö–£–ö–£–†–£–ó–´")
    print("=" * 50)
    
    image_path = "photo/images.jpeg"
    
    if not Path(image_path).exists():
        print(f"‚ùå –§–∞–π–ª {image_path} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # –°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
    classifier = TrainedCornClassifier()
    
    if classifier.model is None:
        print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        return
    
    print(f"\nüì∏ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞: {image_path}")
    print("üéØ –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –∫—É–∫—É—Ä—É–∑–∞")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    result = classifier.predict(image_path)
    
    if result is None:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ!")
        return
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø:")
    print(f"   üåæ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫—É–ª—å—Ç—É—Ä–∞: {result['predicted_class_ru']}")
    print(f"   üìà –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.1%}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å
    is_correct = result['predicted_class'] == 'corn'
    status = "‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û!" if is_correct else "‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û!"
    print(f"   {status}")
    
    print(f"\nüîç –î–ï–¢–ê–õ–¨–ù–´–ï –í–ï–†–û–Ø–¢–ù–û–°–¢–ò:")
    for class_name, prob in result['probabilities'].items():
        class_ru = {'wheat': '–ø—à–µ–Ω–∏—Ü–∞', 'corn': '–∫—É–∫—É—Ä—É–∑–∞', 'barley': '—è—á–º–µ–Ω—å'}[class_name]
        percentage = prob * 100
        print(f"   {class_ru}: {percentage:.1f}%")
    
    print(f"\nüìä –í–ò–ó–£–ê–õ–¨–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò:")
    for feature, value in result['visual_features'].items():
        print(f"   {feature}: {value:.3f}")
    
    return is_correct

if __name__ == "__main__":
    test_corn_image() 